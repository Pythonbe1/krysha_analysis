{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "844a765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 342/342\r"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from itertools import groupby\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_url(url,save_path):\n",
    "    result=[]\n",
    "    count=0\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.text,'lxml')\n",
    "    try:\n",
    "        pages=int(soup.find('nav',class_='paginator').find_all('a',attrs={'class': lambda e: e.endswith('paginator__btn') if e else False})[-2].text.strip())\n",
    "    except:\n",
    "        pages=1\n",
    "    for page in range(1,pages+1):\n",
    "#     for page in range(1,2):\n",
    "        response2=requests.get(f'{url}?das[rent.square]=2&page={page}')\n",
    "        soup2=BeautifulSoup(response2.text,'lxml') \n",
    "        links=soup2.find('section',class_='a-list a-search-list a-list-with-favs').find_all('div',class_='a-card__inc')\n",
    "        for link in links:\n",
    "            result.append(f\"https://krisha.kz/{link.find('a').get('href')}\")\n",
    "        count+=1\n",
    "        print(f'Done: {count}'+f'/{pages}',end='\\r')\n",
    "    with open(f'{save_path}/main_links.json','w',encoding='utf-8') as file:\n",
    "        json.dump(result,file,indent=4,ensure_ascii=False)        \n",
    "\n",
    "def get_data(path):\n",
    "    header={}\n",
    "    save_path='/'.join(path.split('\\\\')[:-1])\n",
    "    data = json.load(open(path,'r', encoding='utf-8'))\n",
    "    result=[]    \n",
    "    count=0\n",
    "    for link in data:\n",
    "        head=[]\n",
    "        value=[]\n",
    "        parameters=[]\n",
    "        response=requests.get(link)\n",
    "        soup=BeautifulSoup(response.text,'lxml')\n",
    "        title=soup.find('h1').text.strip()\n",
    "        price=soup.find('div',class_='offer__price').text.strip()\n",
    "        headings=soup.find('div',class_='offer__short-description').find_all('div',class_=\"offer__info-item\")\n",
    "        for heading in headings:\n",
    "            head.append(heading.find('div').text.strip())\n",
    "            value.append(heading.find('div',class_='offer__advert-short-info').text.strip())\n",
    "        \n",
    "        for i in range(len(head)):\n",
    "            parameters.append({\n",
    "                f'{head[i].strip()}':f'{value[i].strip()}'\n",
    "                 })\n",
    "        result.append({\n",
    "            'title':title,            \n",
    "            'price':price,\n",
    "            'parameters':parameters,\n",
    "            'link':link\n",
    "        }) \n",
    "        count+=1\n",
    "        time.sleep(random.randint(2, 4))\n",
    "        print(f'Done: {count}'+f'/{len(data)}',end='\\r')\n",
    "        if count%10==0:\n",
    "            time.sleep(5)\n",
    "        if len(result)%10==0:\n",
    "            with open(f'{save_path}\\\\result.json','w',encoding='utf-8') as file:\n",
    "                json.dump(result,file,indent=4,ensure_ascii=False)           \n",
    "    \n",
    "    with open(f'{save_path}\\\\result.json','w',encoding='utf-8') as file:\n",
    "        json.dump(result,file,indent=4,ensure_ascii=False)      \n",
    "        \n",
    "\n",
    "def convert_to_excel(path):\n",
    "    save_path='/'.join(path.split('\\\\')[:-1])\n",
    "    data = json.load(open(path,'r', encoding='utf-8'))\n",
    "    result=pd.json_normalize(data)\n",
    "    temp_result=pd.DataFrame()\n",
    "    for i in range(len(result)):\n",
    "        temp=pd.DataFrame(np.diag(pd.DataFrame(result.parameters[i])),index=pd.DataFrame(result.parameters[i]).columns).T\n",
    "        temp_result=temp_result.append(temp,ignore_index=True)\n",
    "    result=pd.concat([result, temp_result], axis=1)\n",
    "    result['price']=result['price'].apply(lambda x: unidecode.unidecode(x.split('〒')[0]))\n",
    "    result['address']=result['title'].apply(lambda x: x.split('м²,')[1] if len(x.split('м²,'))==2 else x)\n",
    "    result['address']=result['address'].apply(lambda x: x.split('—')[0] if len(x.split('—'))==2 else x)\n",
    "    lll = ['title','address','price','Цена в месяц','Общая площадь, м²', 'Город', 'Местоположение',\n",
    "    'Телефон','Безопасность', 'Интернет', 'Название торгового центра, рынка','Что именно?','link']\n",
    "    col = list(result.columns)\n",
    "    ttt = []\n",
    "    for i in lll:\n",
    "        if i in col: ttt.append(i)\n",
    "    result = result[ttt]\n",
    "    result.to_excel(f'{save_path}\\\\data.xlsx',index=False) \n",
    "    \n",
    "def find_coordinates(path):\n",
    "    save_path='/'.join(path.split('\\\\')[:-1])\n",
    "    data=pd.read_excel(path)\n",
    "    result=[]\n",
    "    driver = webdriver.Chrome(executable_path=r'C:\\Users\\svnduw\\.wdm\\drivers\\chromedriver\\win32\\103.0.5060.53\\chromedriver.exe')\n",
    "    driver.get('https://yandex.kz/maps/162/almaty/')\n",
    "    count=0\n",
    "    for index,row in data.iterrows():\n",
    "        searchBox=driver.find_element_by_class_name('input__control')    \n",
    "        searchBox.send_keys(str(row['address']))   \n",
    "        searchBox.send_keys(Keys.ENTER) \n",
    "        time.sleep(3) \n",
    "        y=driver.current_url\n",
    "        y_lon=y.split('?ll=')[1].split('%2C')[0]\n",
    "        y_lat=y.split('?ll=')[1].split('%2C')[1].split('&')[0]\n",
    "        data.at[index,'lat']=y_lat\n",
    "        data.at[index,'lon']=y_lon        \n",
    "        for m in range(len(row['address'])+5):\n",
    "                searchBox.send_keys(Keys.BACK_SPACE)\n",
    "        count+=1\n",
    "        print(f'Done: {count}'+f'/{len(data)}',end='\\r')\n",
    "        if count%10==0:\n",
    "            data.to_excel(f'{save_path}\\\\result.xlsx',index=False)\n",
    "    data.to_excel(f'{save_path}\\\\result.xlsx',index=False)\n",
    "        \n",
    "    \n",
    "\n",
    "def main():\n",
    "    city='almaty'\n",
    "    type_arenda='magazina' #magazina,kvartiry\n",
    "    url=f'https://krisha.kz/arenda/{type_arenda}/{city}/'\n",
    "    save_path=r'C:\\Users\\svnduw\\Desktop\\Bekbol\\Bekbol project\\Новая папка\\parsing'\n",
    "#     get_url(url,save_path)\n",
    "#     get_data(f'{save_path}\\\\main_links.json')\n",
    "#     convert_to_excel(f'{save_path}\\\\result.json')\n",
    "    find_coordinates(f'{save_path}\\\\data.xlsx')\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "     main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff02f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
